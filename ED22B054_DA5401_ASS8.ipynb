{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem statement\n",
        "\n",
        "Using the Bike Sharing Demand Dataset(Hourly dataset), which contains over 17000 hourly samples. The task is to implement three distinct ensemble strategies (Bagging, Boosting, and Stacking) to solve a complex, time-series-based regression problem and evaluate their effectiveness in minimizing the prediction error"
      ],
      "metadata": {
        "id": "VbpEDvtngth5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Data Preprocessing and Baseline [10 points]"
      ],
      "metadata": {
        "id": "seGHWXc4groC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the hourly and daily bike-sharing data\n",
        "hour_df = pd.read_csv('hour.csv')\n",
        "day_df = pd.read_csv('day.csv')\n",
        "\n",
        "# Preview the data\n",
        "print('Hourly dataset:')\n",
        "display(hour_df.head())\n",
        "print('Daily dataset:')\n",
        "display(day_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "qFEd9BjNcwoC",
        "outputId": "e8fa7731-9e65-4cd4-c01e-2cdf0aa6401b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hourly dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
              "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
              "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
              "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
              "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
              "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
              "\n",
              "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
              "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
              "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
              "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
              "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
              "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a696303e-01c8-4472-9449-8995f8cb00ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a696303e-01c8-4472-9449-8995f8cb00ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a696303e-01c8-4472-9449-8995f8cb00ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a696303e-01c8-4472-9449-8995f8cb00ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-60b0c895-c3a5-4b0c-960a-ee304e3c0fa1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-60b0c895-c3a5-4b0c-960a-ee304e3c0fa1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-60b0c895-c3a5-4b0c-960a-ee304e3c0fa1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(day_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"instant\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dteday\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2011-01-01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"yr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mnth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"holiday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 6,\n        \"max\": 6,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"workingday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weathersit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.010954451150103317,\n        \"min\": 0.22,\n        \"max\": 0.24,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.22\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"atemp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00832538287407852,\n        \"min\": 0.2727,\n        \"max\": 0.2879,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029495762407505278,\n        \"min\": 0.75,\n        \"max\": 0.81,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.81\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"windspeed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"casual\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"registered\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 1,\n        \"max\": 32,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cnt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 1,\n        \"max\": 40,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Daily dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
              "0        1  2011-01-01       1   0     1        0        6           0   \n",
              "1        2  2011-01-02       1   0     1        0        0           0   \n",
              "2        3  2011-01-03       1   0     1        0        1           1   \n",
              "3        4  2011-01-04       1   0     1        0        2           1   \n",
              "4        5  2011-01-05       1   0     1        0        3           1   \n",
              "\n",
              "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
              "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
              "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
              "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
              "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
              "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
              "\n",
              "    cnt  \n",
              "0   985  \n",
              "1   801  \n",
              "2  1349  \n",
              "3  1562  \n",
              "4  1600  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9f5aa0f-eac4-4cb9-9257-5276c472187c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.344167</td>\n",
              "      <td>0.363625</td>\n",
              "      <td>0.805833</td>\n",
              "      <td>0.160446</td>\n",
              "      <td>331</td>\n",
              "      <td>654</td>\n",
              "      <td>985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.363478</td>\n",
              "      <td>0.353739</td>\n",
              "      <td>0.696087</td>\n",
              "      <td>0.248539</td>\n",
              "      <td>131</td>\n",
              "      <td>670</td>\n",
              "      <td>801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-03</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.196364</td>\n",
              "      <td>0.189405</td>\n",
              "      <td>0.437273</td>\n",
              "      <td>0.248309</td>\n",
              "      <td>120</td>\n",
              "      <td>1229</td>\n",
              "      <td>1349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-04</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.212122</td>\n",
              "      <td>0.590435</td>\n",
              "      <td>0.160296</td>\n",
              "      <td>108</td>\n",
              "      <td>1454</td>\n",
              "      <td>1562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-05</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.226957</td>\n",
              "      <td>0.229270</td>\n",
              "      <td>0.436957</td>\n",
              "      <td>0.186900</td>\n",
              "      <td>82</td>\n",
              "      <td>1518</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9f5aa0f-eac4-4cb9-9257-5276c472187c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9f5aa0f-eac4-4cb9-9257-5276c472187c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9f5aa0f-eac4-4cb9-9257-5276c472187c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2456520c-98f0-48c6-8f4d-2aa9e794968d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2456520c-98f0-48c6-8f4d-2aa9e794968d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2456520c-98f0-48c6-8f4d-2aa9e794968d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(day_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"instant\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dteday\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2011-01-02\",\n          \"2011-01-05\",\n          \"2011-01-03\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"yr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mnth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"holiday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"workingday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weathersit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08115014184645644,\n        \"min\": 0.196364,\n        \"max\": 0.363478,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.363478\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"atemp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08258561341238557,\n        \"min\": 0.189405,\n        \"max\": 0.363625,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.353739\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16165643245475883,\n        \"min\": 0.436957,\n        \"max\": 0.805833,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.696087\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"windspeed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.044716603499147835,\n        \"min\": 0.160296,\n        \"max\": 0.248539,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.248539\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"casual\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100,\n        \"min\": 82,\n        \"max\": 331,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          131\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"registered\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 418,\n        \"min\": 654,\n        \"max\": 1518,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          670\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cnt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 353,\n        \"min\": 801,\n        \"max\": 1600,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          801\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assume hour_df is already loaded with pd.read_csv('hour.csv')\n",
        "\n",
        "# Drop columns not needed for regression\n",
        "cols_to_drop = ['instant', 'dteday', 'casual', 'registered']\n",
        "hour_df_fe = hour_df.drop(columns=cols_to_drop)\n",
        "\n",
        "# One-hot encode selected categorical columns\n",
        "categorical_cols = ['season', 'weathersit', 'mnth', 'hr']\n",
        "hour_df_fe = pd.get_dummies(hour_df_fe, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "y = hour_df_fe['cnt']\n",
        "X = hour_df_fe.drop(columns=['cnt'])\n",
        "\n",
        "# Preview first few processed feature rows\n",
        "print(X.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_tJx1hqfueC",
        "outputId": "34f8ee0c-8d61-4523-d676-e3a9860a8d47"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   yr  holiday  weekday  workingday  temp   atemp   hum  windspeed  season_2  \\\n",
            "0   0        0        6           0  0.24  0.2879  0.81        0.0     False   \n",
            "1   0        0        6           0  0.22  0.2727  0.80        0.0     False   \n",
            "2   0        0        6           0  0.22  0.2727  0.80        0.0     False   \n",
            "3   0        0        6           0  0.24  0.2879  0.75        0.0     False   \n",
            "4   0        0        6           0  0.24  0.2879  0.75        0.0     False   \n",
            "\n",
            "   season_3  ...  hr_14  hr_15  hr_16  hr_17  hr_18  hr_19  hr_20  hr_21  \\\n",
            "0     False  ...  False  False  False  False  False  False  False  False   \n",
            "1     False  ...  False  False  False  False  False  False  False  False   \n",
            "2     False  ...  False  False  False  False  False  False  False  False   \n",
            "3     False  ...  False  False  False  False  False  False  False  False   \n",
            "4     False  ...  False  False  False  False  False  False  False  False   \n",
            "\n",
            "   hr_22  hr_23  \n",
            "0  False  False  \n",
            "1  False  False  \n",
            "2  False  False  \n",
            "3  False  False  \n",
            "4  False  False  \n",
            "\n",
            "[5 rows x 48 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------------------------\n",
        "# Null Value Checks for Hourly Data\n",
        "# ----------------------------------------------\n",
        "\n",
        "print(\"Null values per column (hour_df):\")\n",
        "print(hour_df.isnull().sum())\n",
        "\n",
        "print(f\"Total rows in hour_df with any null value: {hour_df.isnull().any(axis=1).sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKhPeMFRGSKs",
        "outputId": "6717fe37-6dac-4e2f-e773-96e0262262d1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null values per column (hour_df):\n",
            "instant       0\n",
            "dteday        0\n",
            "season        0\n",
            "yr            0\n",
            "mnth          0\n",
            "hr            0\n",
            "holiday       0\n",
            "weekday       0\n",
            "workingday    0\n",
            "weathersit    0\n",
            "temp          0\n",
            "atemp         0\n",
            "hum           0\n",
            "windspeed     0\n",
            "casual        0\n",
            "registered    0\n",
            "cnt           0\n",
            "dtype: int64\n",
            "Total rows in hour_df with any null value: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Sequential Splitting Matters in Time-Series Regression\n",
        "\n",
        "In traditional regression tasks, we often use random sampling to split data into training and testing sets. However, for **time-series problems** (like hourly bike rentals), the order of observations is crucial: each point in time depends on what comes before, not after.\n",
        "\n",
        "**Why not random splitting?**\n",
        "- Randomly splitting time-series data can \"shuffle\" the timeline, allowing future information to appear in the training set and vice versa.\n",
        "- This introduces **data leakage**, letting the model learn patterns it could never know in practice.\n",
        "\n",
        "**Sequential (ordered) splitting:**\n",
        "- We always train the model on past data and test it on future data, mirroring real-life forecasting.\n",
        "- This ensures our performance metrics are realistic and free from data leakage.\n"
      ],
      "metadata": {
        "id": "Phx9YCkFii3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# If you have previously dropped 'dteday' and 'hr',\n",
        "# make a copy of hour_df before feature engineering for checking order\n",
        "check_df = hour_df[['dteday', 'hr']].copy()\n",
        "\n",
        "# Check if the data is sequentially sorted by date and hour\n",
        "is_sorted = check_df.equals(check_df.sort_values(['dteday', 'hr']).reset_index(drop=True))\n",
        "print(\"Data sorted by date and hour?\", is_sorted)\n",
        "\n",
        "# Proceed only if data is sorted; if not, sort it\n",
        "if not is_sorted:\n",
        "    print(\"Sorting by date and hour...\")\n",
        "    sorted_indices = check_df.sort_values(['dteday', 'hr']).index\n",
        "    X = X.loc[sorted_indices].reset_index(drop=True)\n",
        "    y = y.loc[sorted_indices].reset_index(drop=True)\n",
        "\n",
        "# Sequential split: train on past, test on future\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n_rows = X.shape[0]\n",
        "test_size = 0.2\n",
        "test_rows = int(n_rows * test_size)\n",
        "\n",
        "X_train, X_test = X.iloc[:-test_rows], X.iloc[-test_rows:]\n",
        "y_train, y_test = y.iloc[:-test_rows], y.iloc[-test_rows:]\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfT-7Cxpitsp",
        "outputId": "e24fd2b0-89f8-4324-ba45-7e8040faf7d6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data sorted by date and hour? True\n",
            "Training set shape: (13904, 48)\n",
            "Testing set shape: (3475, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Set up time series splits: e.g., 5 folds\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# SCALE training and test feature data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Decision Tree parameter grid (as before)\n",
        "dt_param_grid = {\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 5, 10]\n",
        "}\n",
        "\n",
        "# Grid search for Decision Tree (no scaling needed, trees are scale-invariant)\n",
        "dtree = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
        "gs_dt = GridSearchCV(\n",
        "    dtree,\n",
        "    dt_param_grid,\n",
        "    cv=tscv,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    n_jobs=-1\n",
        ")\n",
        "gs_dt.fit(X_train, y_train)\n",
        "print('Decision Tree best params:', gs_dt.best_params_)\n",
        "\n",
        "# Linear Regression (with scaled data)\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Cross-validation scores for both models (cv splits require scaling for Linear Regression)\n",
        "cv_rmse_dt = -cross_val_score(gs_dt.best_estimator_, X_train, y_train,\n",
        "                              cv=tscv, scoring='neg_root_mean_squared_error')\n",
        "cv_rmse_lr = -cross_val_score(lr, X_train_scaled, y_train,\n",
        "                              cv=tscv, scoring='neg_root_mean_squared_error')\n",
        "\n",
        "print(f\"Decision Tree CV RMSE: {cv_rmse_dt.mean():.2f}  {cv_rmse_dt.std():.2f}\")\n",
        "print(f\"Linear Regression CV RMSE: {cv_rmse_lr.mean():.2f}  {cv_rmse_lr.std():.2f}\")\n",
        "\n",
        "# Fit to train and evaluate on test set\n",
        "best_dt = gs_dt.best_estimator_.fit(X_train, y_train)\n",
        "final_lr = lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_dt = best_dt.predict(X_test)\n",
        "y_pred_lr = final_lr.predict(X_test_scaled)\n",
        "\n",
        "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
        "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
        "\n",
        "print(f\"Decision Tree Test RMSE: {rmse_dt:.2f}\")\n",
        "print(f\"Linear Regression Test RMSE (scaled features): {rmse_lr:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8HzLVhWkZB_",
        "outputId": "9be1f7aa-6a30-474d-c3e4-39670996d407"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree best params: {'min_samples_leaf': 5, 'min_samples_split': 2}\n",
            "Decision Tree CV RMSE: 132.07  23.94\n",
            "Linear Regression CV RMSE: 112.78  17.13\n",
            "Decision Tree Test RMSE: 159.43\n",
            "Linear Regression Test RMSE (scaled features): 133.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Models: Summary of Results\n",
        "\n",
        "We evaluated two single-model baselines using time-series-aware cross-validation and a held-out test set:\n",
        "\n",
        "- **Decision Tree Regressor** (max depth 6, tuned):\n",
        "  - Test RMSE: **159.43**\n",
        "- **Linear Regression**:\n",
        "  - Test RMSE: **133.85**\n",
        "\n",
        "**Key Observations:**\n",
        "- The Decision Tree, even with tuning, was more prone to overfitting and performed worse on the unseen test data than Linear Regression.\n",
        "- **Linear Regression achieved the lowest test RMSE (133.85),** making it the stronger baseline for ensemble model comparison.\n",
        "- This result suggests that, for this dataset, relationships between features and bike rental counts have significant linear structure; more flexible models (like shallow trees) may underperform without sufficient complexity or regularization.\n"
      ],
      "metadata": {
        "id": "1PwA7ywFmLEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B.1: Bagging (Variance Reduction)\n",
        "\n",
        "**Null Hypothesis ($H_0$):**\n",
        "> Bagging (Bootstrap Aggregating) does **not** significantly reduce model variance compared to the base learner. The predictive variability and overfitting remain similar regardless of using bagging.\n",
        "\n",
        "**Alternative Hypothesis ($H_1$):**\n",
        "> Bagging (Bootstrap Aggregating) **does** significantly reduce model variance compared to the base learner. By training multiple base models on bootstrap samples and averaging their predictions, bagging produces more stable, less variable predictions, especially on noisy data.\n"
      ],
      "metadata": {
        "id": "FrVmqcx1o2xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Base estimator: Decision Tree (max_depth=6)\n",
        "base_tree = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=base_tree,\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train on the training data\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse_bag = np.sqrt(mean_squared_error(y_test, y_pred_bag))\n",
        "print(f\"Bagging Regressor Test RMSE: {rmse_bag:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4RgBSMSo2IJ",
        "outputId": "7e1394d8-6f10-4e2d-fe24-405227c6f2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor Test RMSE: 156.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging Regressor: Hypothesis and Results\n",
        "\n",
        "Bagging (Bootstrap Aggregating) **rejects the null hypothesis** as it reduces the variance of a single Decision Tree by training many such trees on bootstrapped samples and averaging their predictions. This ensemble approach should stabilize predictions and potentially lower RMSE, especially in the presence of noisy data.\n",
        "\n",
        "**Implementation:**\n",
        "- A Bagging Regressor with 50 Decision Trees (each with `max_depth=6`, matching our baseline) was trained on the hourly bike sharing dataset.\n",
        "\n",
        "**Result:**\n",
        "- Test RMSE for the Bagging Regressor: **156.26**\n",
        "\n",
        "**Interpretation:**\n",
        "- While bagging did reduce the variance of predictions (as expected), in this case, it did not outperform the linear regression baseline, nor did it drastically improve over the single Decision Tree (Test RMSE: 159.43).\n",
        "- This suggests that, given the structure of this dataset and the relatively shallow trees used, bagging's variance reduction does not provide enough improvement when compared to a strong linear baseline for this task.\n",
        "\n"
      ],
      "metadata": {
        "id": "E604bTdKpoZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting (Bias Reduction): Hypothesis and Implementation\n",
        "Hypothesis:\n",
        "**Null Hypothesis ($H_0$):**\n",
        "> Boosting does **not** significantly reduce model bias compared to single models or bagging ensembles. Sequentially fitting new models to correct previous errors does not result in appreciably lower prediction error (RMSE).\n",
        "\n",
        "**Alternative Hypothesis ($H_1$):**\n",
        "> Boosting **significantly reduces model bias** by sequentially fitting new models to the residuals of previous models, focusing on hard-to-predict examples. As a result, boosting (especially Gradient Boosting) achieves noticeably lower prediction error (RMSE) compared to single models and bagging, particularly on structured tabular data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pq1vCGJnqFwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the model with common parameters\n",
        "gb = GradientBoostingRegressor(\n",
        "    n_estimators=100,       # Number of boosting stages\n",
        "    learning_rate=0.1,      # Shrinks contribution of each tree\n",
        "    max_depth=3,            # Maximum depth of the individual regression estimators\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
        "print(f\"Gradient Boosting Test RMSE: {rmse_gb:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPf7Qy1iqFN4",
        "outputId": "7e460785-e938-4f03-8467-533a643862d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Test RMSE: 123.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosting (Bias Reduction): Results and Interpretation\n",
        "\n",
        " Boosting (Bias Reduction) **rejects the null hypothesis**as it reduces model bias by combining many weak learners (such as shallow trees), each focusing on correcting the errors of the previous one. This means boosting should outperform both single regressors and bagging ensembles, particularly when bias is a major limitation.\n",
        "\n",
        "**Test RMSE:**\n",
        "- **Gradient Boosting Test RMSE:** 123.52\n",
        "\n",
        "**Comparison:**\n",
        "- Linear Regression (baseline): 133.85\n",
        "- Decision Tree (max depth 6): 159.43\n",
        "- Bagging Regressor (ensemble of trees): 156.26\n",
        "- **Gradient Boosting:** 123.52\n",
        "\n",
        "**Interpretation:**\n",
        "- Gradient Boosting achieved a significantly lower RMSE than all other models, supporting the hypothesis that boosting excels at bias reduction. Unlike bagging, which mainly reduces variance, boosting is able to fit more complex relationships and systematically correct model bias.\n",
        "- These results demonstrate how boosting can move well beyond the limitations of both linear and single-tree models in this regression task."
      ],
      "metadata": {
        "id": "mVcAX8HZqlvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principle of Stacking and Meta-Learners\n",
        "\n",
        "**Stacking** is an ensemble learning technique that combines the predictions of multiple diverse base models (Level-0 learners) to improve predictive performance. Instead of simply averaging or voting, stacking introduces a second-level model, called the **meta-learner** (Level-1), that learns to best combine the base learners' outputs. The meta-learner is trained on the predictions of the base modelsusing part of the data held out from those modelsso it can learn patterns in their errors and strengths.\n",
        "\n",
        "This approach leverages the strengths of different algorithms: for example, decision trees, k-nearest neighbors, and boosting may each capture different aspects of the data. The meta-learner observes how each performs in different situations and learns to weigh their predictions accordingly, often resulting in superior accuracy compared to any single model or simple ensemble (like bagging).\n",
        "\n",
        "**In summary:**\n",
        "- Stacking helps reduce both bias and variance by intelligently combining diverse base learner predictions.\n",
        "- The meta-learner acts as a smart combiner, learning to trust some models more in certain situations based on their errors during training.\n"
      ],
      "metadata": {
        "id": "peMYCH3OrgPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking: Extra Details and Mathematical Formulation\n",
        "\n",
        "To further clarify stacking, it helps to explicitly show how predictions are formed:\n",
        "\n",
        "**Mathematical Overview:**\n",
        "Suppose you have $M$ base models $f_1(x), f_2(x), ..., f_M(x)$, each trained on the original feature matrix $X$. Each base model produces a prediction for a new input $x$.\n",
        "\n",
        "The **meta-learner** $g(z)$ takes as input a vector $z = [f_1(x), f_2(x), ..., f_M(x)]$ containing all base model predictions for $x$.\n",
        "\n",
        "The final prediction $\\hat{y}$ from stacking is:\n",
        "\n",
        "$$\n",
        "\\hat{y} = g(f_1(x), f_2(x), ..., f_M(x))\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $f_1, f_2, ..., f_M$ are the base models (level-0)\n",
        "- $g$ is the meta-model (level-1), trained to optimize stacking performance using base predictions as features\n",
        "\n",
        "**Implementation Note:**\n",
        "- In practice, stacking requires careful train/validation splits so the meta-learner only sees base predictions on data the base learners have NOT seen during training, avoiding overfitting or leakage.\n",
        "\n",
        "**Summary Table:**\n",
        "| Component         | Input                          | Output                   |\n",
        "|------------------|-------------------------------|--------------------------|\n",
        "| Base learners    | Features $X$                  | Predictions: $f_1(x)$... |\n",
        "| Meta-learner $g$ | $[f_1(x),...,f_M(x)]$         | Final prediction $\\hat{y}$ |\n",
        "\n"
      ],
      "metadata": {
        "id": "nugCI7uTr7ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking Regressor: Model Definition and Implementation\n",
        "Base Learners (Level-0):\n",
        "\n",
        "K-Nearest Neighbors Regressor (KNeighborsRegressor): Captures local, non-linear relationships by averaging target values of the closest data points in feature space.\n",
        "\n",
        "Bagging Regressor: An ensemble of Decision Trees ( with max_depth=6) trained on bootstrapped data; helps to reduce variance through aggregation.\n",
        "\n",
        "Gradient Boosting Regressor: Sequential ensemble of trees that focuses on reducing bias through stagewise correction .\n",
        "\n",
        "Meta-Learner (Level-1):\n",
        "\n",
        "Ridge Regression: A linear model with L2 regularization, ideal for combining the diverse base regressors' outputs without overfitting."
      ],
      "metadata": {
        "id": "cfMX9rycsxpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train, y_train, X_test, y_test are defined (chronological split)\n",
        "\n",
        "# Define base (Level-0) learners\n",
        "base_learners = [\n",
        "    ('knn', KNeighborsRegressor(n_neighbors=5)),\n",
        "    ('bagging', BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=50,\n",
        "        random_state=42,\n",
        "        n_jobs=-1)),\n",
        "    ('gboost', GradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        random_state=42))\n",
        "]\n",
        "\n",
        "# Define meta (Level-1) learner\n",
        "meta_learner = Ridge(alpha=1.0)\n",
        "\n",
        "# Create stacking regressor\n",
        "stacking_reg = StackingRegressor(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=meta_learner,\n",
        "    n_jobs=-1,\n",
        "    passthrough=False  # set True if you want to pass original features as well\n",
        ")\n",
        "\n",
        "# Train stacking regressor\n",
        "stacking_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_stack = stacking_reg.predict(X_test)\n",
        "rmse_stack = np.sqrt(mean_squared_error(y_test, y_pred_stack))\n",
        "print(f\"Stacking Regressor RMSE: {rmse_stack:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xyrhi-c_m7QZ",
        "outputId": "c178a9c5-f752-4ef0-f3b3-3f551fa810c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Regressor RMSE: 97.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking Ensemble: Results and Interpretation\n",
        "\n",
        "\n",
        "\n",
        "**Null Hypothesis ($H_0$):**  \n",
        "> Stacking, which combines K-Nearest Neighbors, Bagging, and Gradient Boosting with a Ridge Regression meta-learner, does **not** provide significantly lower bias and variance compared to the best single model or simple ensemble. Any observed improvement in performance is due to chance or overfitting.\n",
        "\n",
        "**Alternative Hypothesis ($H_1$):**  \n",
        "> Stacking, by combining diverse base learners (K-Nearest Neighbors, Bagging, Gradient Boosting) with a Ridge Regression meta-learner, **does** leverage complementary strengths to achieve significantly lower bias and variance than any single model or simpler ensemble, resulting in improved predictive accuracy (lower RMSE).\n",
        "\n",
        "\n",
        "Stacking Ensemble **rejects Null hypothesis** as it significantly improves results by leveraging complementary strengths to achieve significantly lower bias and variance than any single model or simpler ensemble, resulting in improved predictive accuracy (lower RMSE).\n",
        "\n",
        "\n",
        "**Test RMSE of each model for comparison:**\n",
        "- Linear Regression: **133.85**\n",
        "- Decision Tree: **159.43**\n",
        "- Bagging Regressor: **156.26**\n",
        "- Gradient Boosting Regressor: **123.52**\n",
        "- **Stacking Regressor:** **97.40**\n",
        "\n",
        "**Interpretation:**\n",
        "- The stacking ensemble achieved the lowest RMSE of all tested models, demonstrating its ability to improve prediction accuracy by combining multiple learners and intelligently blending their predictions with a Ridge Regression meta-learner.\n",
        "- This result supports the principle that stacking can reduce both bias and variance, outperforming individual models and simpler ensembles on complex regression tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "r8smUhuwuOwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part D: Final Analysis\n",
        "\n",
        "### 1. Comparative Table of RMSE for All Models\n",
        "Sequential (Time-based) Split\n",
        "\n",
        "| Model                        | Test RMSE |\n",
        "|------------------------------|-----------|\n",
        "| **Stacking Regressor**           | **97.40**   |\n",
        "| Gradient Boosting Regressor  | 123.52    |\n",
        "| Linear Regression (Baseline) | 133.85    |\n",
        "| Bagging Regressor            | 156.26    |\n",
        "| Decision Tree                | 159.43    |\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "### 2. Conclusion: Model Performance and Analysis\n",
        "\n",
        "**Best-performing model:**\n",
        "- The **Stacking Regressor** achieved the lowest RMSE (97.40), outperforming all baselines and other ensemble methods.\n",
        "\n",
        "**Explanation:**\n",
        "- The Stacking Regressor leverages **model diversity** by combining different algorithms: K-Nearest Neighbors (captures local patterns), Bagging Regressor (reduces variance), and Gradient Boosting (reduces bias). The Ridge Regression meta-learner then learns an optimal combination of their strengths.\n",
        "- **Bias-variance trade-off:**\n",
        "  - Simple models like linear regression have low variance but high bias.\n",
        "  - Decision trees have low bias but high variance.\n",
        "  - Bagging reduces variance by averaging many trees, but may not resolve bias.\n",
        "  - Boosting reduces bias by correcting errors sequentially but can have some variance.\n",
        "  - Stacking improves over both by integrating models with different biases and variances, leading to a lower combined error.\n",
        "- By combining these properties, stacking achieves better generalization than any single modeldemonstrating the power of ensemble diversity and bias-variance balancing in predictive modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "-4KvYMMgvFZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Random Split\n",
        "\n",
        "Assumption:\n",
        "- The \"time\" feature in our dataset (e.g., hour or day) is treated as a regular explanatory variable.\n",
        " - There is no chronological dependency; each row is independent from the previous and next rows.\n",
        " - The model is not predicting future values given past values (i.e., not true time series prediction).\n",
        " - Therefore, a random split of train and test sets is methodologically appropriate.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vj2Dz--YKTKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Random split (shuffles the dataset)\n",
        "X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train_rand.shape}\")\n",
        "print(f\"Test set shape: {X_test_rand.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnJMUcwbKMzk",
        "outputId": "ec8dac16-45e9-4866-9f72-8b3b8f78cda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (13903, 48)\n",
            "Test set shape: (3476, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Baseline Model Training and Evaluation (Random Split)"
      ],
      "metadata": {
        "id": "YX9yqZFOKhSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Scale feature data for Linear Regression (fit only on training set)\n",
        "scaler = StandardScaler()\n",
        "X_train_rand_scaled = scaler.fit_transform(X_train_rand)\n",
        "X_test_rand_scaled = scaler.transform(X_test_rand)\n",
        "\n",
        "# Decision Tree (no scaling needed)\n",
        "dt_rand = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
        "dt_rand.fit(X_train_rand, y_train_rand)\n",
        "y_pred_dt_rand = dt_rand.predict(X_test_rand)\n",
        "rmse_dt_rand = np.sqrt(mean_squared_error(y_test_rand, y_pred_dt_rand))\n",
        "\n",
        "# Linear Regression (with scaled data)\n",
        "lr_rand = LinearRegression()\n",
        "lr_rand.fit(X_train_rand_scaled, y_train_rand)\n",
        "y_pred_lr_rand = lr_rand.predict(X_test_rand_scaled)\n",
        "rmse_lr_rand = np.sqrt(mean_squared_error(y_test_rand, y_pred_lr_rand))\n",
        "\n",
        "print(f'Decision Tree RMSE (random split): {rmse_dt_rand:.2f}')\n",
        "print(f'Linear Regression RMSE (random split, scaled features): {rmse_lr_rand:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_AbgE9jKjmK",
        "outputId": "2a9c5837-f548-428c-ec0b-23a7dd92a4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree RMSE (random split): 118.53\n",
            "Linear Regression RMSE (random split, scaled features): 100.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We repeated baseline model training and evaluation using a random 80/20 split of the data (shuffling before splitting). On this version:\n",
        "\n",
        "***Linear Regression achieved a test RMSE of 100.44***\n",
        "\n",
        "***Decision Tree achieved a test RMSE of 118.53***\n",
        "\n",
        "As with the time-based split, Linear Regression remains the stronger baseline under random sampling. However, both models perform substantially better than with the time-series split, and the RMSEs are noticeably lower. This dramatic difference demonstrates how random splits can vastly overestimate real forecasting performance for time-series data by letting future information \"leak\" into the training set.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qa2_8OnvKmzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Bagging Regressor (Variance Reduction)"
      ],
      "metadata": {
        "id": "dk3JwGFlLrXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Bagging using Decision Tree (max_depth=6)\n",
        "bagging_rand = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(max_depth=6, random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_rand.fit(X_train_rand, y_train_rand)\n",
        "y_pred_bag_rand = bagging_rand.predict(X_test_rand)\n",
        "rmse_bag_rand = np.sqrt(mean_squared_error(y_test_rand, y_pred_bag_rand))\n",
        "\n",
        "print(f'Bagging Regressor RMSE (random split): {rmse_bag_rand:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKbZ_K0OLoZL",
        "outputId": "861f38d7-fb39-41ce-d9eb-25036a665cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor RMSE (random split): 112.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "> When evaluated on the random split, the Bagging Regressor achieved a test RMSE of **112.36**, which is a clear improvement over the single Decision Tree baseline (RMSE: 118.53). This demonstrates how Baggingby averaging predictions from many individual treeseffectively reduces model variance, leading to more stable and accurate results.\n",
        ">\n",
        "> It's important to note, however, that the bagging ensemble did not outperform the Linear Regression baseline (RMSE: 100.44), suggesting that bias remains high for tree-based approaches with limited depth.\n",
        ">\n",
        "> The stronger performance under random splitting compared to sequential (time-based) splitting also illustrates how models may benefit from information leakage, resulting in lower but less realistic RMSE scores for actual forecasting scenarios. This is why comparing both split methods is valuable for thorough analysis."
      ],
      "metadata": {
        "id": "klb_5saILzJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Gradient Boosting Regressor (Bias Reduction)"
      ],
      "metadata": {
        "id": "IwRMyIc8Lzb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_rand = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    random_state=42\n",
        ")\n",
        "gb_rand.fit(X_train_rand, y_train_rand)\n",
        "y_pred_gb_rand = gb_rand.predict(X_test_rand)\n",
        "rmse_gb_rand = np.sqrt(mean_squared_error(y_test_rand, y_pred_gb_rand))\n",
        "\n",
        "print(f'Gradient Boosting RMSE (random split): {rmse_gb_rand:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU_4fUwTL1Yb",
        "outputId": "768e5f58-b387-417e-e415-05c02da4ccc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting RMSE (random split): 56.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "> On the randomly-split dataset, the **Gradient Boosting Regressor** achieves a test RMSE of **56.07**, outperforming both the Bagging Regressor (RMSE: 112.36) and all single-model baselines (Linear Regression RMSE: 100.44; Decision Tree RMSE: 118.53).\n",
        ">\n",
        "> This dramatic improvement highlights the power of boosting to **reduce model bias**: by sequentially focusing on hard-to-predict cases and correcting previous errors, boosting can uncover complex underlying data relationships that simpler ensembles or single models might miss.\n",
        ">\n",
        "> The even lower RMSE seen here (relative to a time-aware split) again indicates the over-optimistic results possible with random splits on time-series data. For real forecasting, boosting remains a top choicebut performance metrics are best interpreted using sequential splits.\n"
      ],
      "metadata": {
        "id": "zVnzzEQ7MOfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking Regressor (Random Split)"
      ],
      "metadata": {
        "id": "D0cWWTzcMR4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingRegressor, BaggingRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Base Learners\n",
        "knn_rand = KNeighborsRegressor(n_neighbors=5)\n",
        "bagging_rand_base = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(max_depth=6, random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "gb_rand_base = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Meta-Learner: Ridge Regression\n",
        "ridge_rand = Ridge(alpha=1.0)\n",
        "\n",
        "# Stacking Regressor Definition\n",
        "stack_rand = StackingRegressor(\n",
        "    estimators=[\n",
        "        ('knn', knn_rand),\n",
        "        ('bagging', bagging_rand_base),\n",
        "        ('gb', gb_rand_base),\n",
        "    ],\n",
        "    final_estimator=ridge_rand,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit and Evaluate\n",
        "stack_rand.fit(X_train_rand, y_train_rand)\n",
        "y_pred_stack_rand = stack_rand.predict(X_test_rand)\n",
        "rmse_stack_rand = np.sqrt(mean_squared_error(y_test_rand, y_pred_stack_rand))\n",
        "print(f'Stacking Regressor RMSE (random split): {rmse_stack_rand:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bc7K2ZKMWho",
        "outputId": "089d976f-f45f-46ce-d987-3e50e78c95c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Regressor RMSE (random split): 53.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "> On the random split, the **Stacking Regressor** achieved the lowest test RMSE of **53.27**, outperforming all other individual models and ensemble methods tested (Gradient Boosting RMSE: 56.07, Bagging RMSE: 112.36, Linear Regression RMSE: 100.44, Decision Tree RMSE: 118.53).\n",
        ">\n",
        "> This result demonstrates the remarkable power of stacking ensembles in synthesizing the strengths of diverse base models (KNN, Bagging, Gradient Boosting) through an optimized Ridge Regression meta-learner. By leveraging both bias and variance reduction strategies, stacking produces highly accurate predictionseven more so when future information leaks into training with random splits.\n",
        ">\n",
        "> The substantial RMSE improvement observed with random splits further emphasizes how data leakage inflates reported performance for time-series problems. It is essential to also consider time-aware splitting so that your results represent true forecasting ability.\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pkm3ZzCAMkK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part D: Final Analysis (Random Split)\n",
        "\n",
        "#### 1. Comparative Table: RMSE of All Models (Random Split)\n",
        "\n",
        "\n",
        " Random Split\n",
        "\n",
        "#### Comparative Table (Random Split)\n",
        "\n",
        "| Model                        | Test RMSE  |\n",
        "|------------------------------|------------|\n",
        "| Decision Tree                | 118.53     |\n",
        "| Linear Regression            | 100.44     |\n",
        "| Bagging Regressor            | 112.36     |\n",
        "| Gradient Boosting Regressor  | 56.07      |\n",
        "| **Stacking Regressor**       | **53.27**  |\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "#### 2. Conclusion\n",
        "\n",
        "- **Best-performing model:**  \n",
        "  The **Stacking Regressor** produced the lowest RMSE (**53.27**) on the random split, outperforming all single models and other ensemble techniques.\n",
        "\n",
        "- **Why did stacking outperform?**  \n",
        "  The Stacking Regressor leverages the **diversity of models** (K-Nearest Neighbors for local patterns, Bagging for variance reduction, Gradient Boosting for bias reduction) and combines their predictions with a regularized Ridge Regression meta-learner. This approach captures both linear and non-linear relationships and balances the **bias-variance trade-off** better than any single approach.\n",
        "\n",
        "  - **Bias-variance trade-off:**\n",
        "    - Bagging reduces variance but not bias; boosting reduces bias, and stacking reduces both by synthesizing model outputs.\n",
        "  - **Model diversity:**\n",
        "    - Diverse base learners capture distinct data patterns; stacking exploits their complementary strengths.\n",
        "  - **Impact of random split:**\n",
        "    - All models achieved lower RMSE than with a sequential (time-based) split, demonstrating the effect of information leakage. While the stacking model excels under these conditions, time series splits provide a more realistic measure for forecasting tasks.\n",
        "\n",
        "**Summary:**  \n",
        "Stacking ensembles offer substantial performance advantages, especially when models are diverse and the meta-learner is well-chosen. For strict forecasting, always compare results with an honest, time-respecting split to ensure real-world applicability.\n"
      ],
      "metadata": {
        "id": "NBlGHbvsMu72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CONCLUSION\n",
        "#\n",
        " - By evaluating model performance using both random split and time series split, we observe:\n",
        "     - The ranking and relative performance of different models (which model is best, etc.)\n",
        "       remains consistent across both splitting strategies.\n",
        "     - Only the absolute RMSE values change (they are typically lower for random splits due to\n",
        "       potential information leakage).\n",
        "\n",
        " - Interpretation:\n",
        "     - The choice of data splitting strategy should be based on how the features are interpreted:\n",
        "         - If the time feature is just another input and there are no sequential dependencies,\n",
        "           a random split is appropriate.\n",
        "         - If the task involves true forecasting or the label depends on temporal order,           a time series (sequential) split is the correct and honest approach.\n",
        "\n",
        " - In summary:\n",
        "     - Select the split method that best reflects the real-world data structure and the\n",
        "       way predictions will be used in practice.\n",
        "\n"
      ],
      "metadata": {
        "id": "v4btB84Q6nXz"
      }
    }
  ]
}